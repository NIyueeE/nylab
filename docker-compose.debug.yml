# docker-compose.debug.yml
version: '3.7'

services:
  # MinIO 对象存储（调试版）
  minio:
    image: minio/minio
    container_name: minio_debug
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ':9001'
    environment:
      MINIO_ROOT_USER: ${AWS_ACCESS_KEY_ID}
      MINIO_ROOT_PASSWORD: ${AWS_SECRET_ACCESS_KEY}
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 3
  
  mc:
    image: minio/mc
    container_name: mc_debug
    env_file:
      - .env
    entrypoint: |
      /bin/sh -c "
      /tmp/wait-for-it.sh minio:9000 &&
      /usr/bin/mc alias set minio http://minio:9000 $${AWS_ACCESS_KEY_ID} $${AWS_SECRET_ACCESS_KEY} &&
      
      # 创建必需存储桶
      (/usr/bin/mc ls minio/mlflow || /usr/bin/mc mb minio/mlflow) &&
      (/usr/bin/mc ls minio/mlflow-datasets || /usr/bin/mc mb minio/mlflow-datasets) &&
      (/usr/bin/mc ls minio/training-scripts || /usr/bin/mc mb minio/training-scripts) &&
      (/usr/bin/mc ls minio/pre-trained-models || /usr/bin/mc mb minio/pre-trained-models);
      
      # 复制 training-scripts 文件到桶
      if [ -d '/tmp/training-scripts' ]; then
        echo '复制 training-scripts 到 MinIO...';
        /usr/bin/mc cp --recursive /tmp/training-scripts/ minio/training-scripts/;
      else
        echo '警告: /tmp/training-scripts 目录不存在';
      fi;
      
      # 复制 pre-trained-models 文件到桶
      if [ -d '/tmp/pre-trained-models' ]; then
        echo '复制 pre-trained-models 到 MinIO...';
        /usr/bin/mc cp --recursive /tmp/pre-trained-models/ minio/pre-trained-models/;
      else
        echo '警告: /tmp/pre-trained-models 目录不存在';
      fi;
      
      exit 0;
      "
    volumes:
      - ./init_minio/wait-for-it.sh:/tmp/wait-for-it.sh
      - ./init_minio/training-scripts:/tmp/training-scripts
      - ./init_minio/pre-trained-models:/tmp/pre-trained-models
    depends_on:
      minio:
        condition: service_healthy

  # MLflow 跟踪服务器
  mlflow:
    build: ./mlflow
    image: mlflow_server
    container_name: mlflow_debug
    ports:
      - "5000:5000"
    environment:
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    command: >
      sh -c "
      mlflow server
      --backend-store-uri sqlite:///mlflow.db
      --default-artifact-root s3://mlflow/
      --host 0.0.0.0

      while ! mc ls minio/mlflow >/dev/null 2>&1; do sleep 1; done;
      "
    depends_on:
      minio:
        condition: service_healthy
      mc:
        condition: service_completed_successfully

  redis:
    image: redis:alpine
    container_name: nylab_redis_debug
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 3

  # 后端API服务
  web:
    build:
      context: .
      dockerfile: ./web/Dockerfile
    container_name: nylab_web_debug
    ports:
      - "8000:8000"   # API端口
      - "5678:5678"   # 调试端口
    volumes:
      # 代码热重载
      - ./web:/app/web  
      - ./backend_common:/app/backend_common

      - shared_data:/data # 与worker服务共享上传数据
    environment:
      PYTHONUNBUFFERED: "1"
      MLFLOW_TRACKING_URI: http://mlflow:5000
      MINIO_ENDPOINT: minio:9000
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      REDIS_HOST: redis
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/1
    command: >
      sh -c "
      python -m debugpy --listen 0.0.0.0:5678 --wait-for-client -m uvicorn web.src.main:app --host 0.0.0.0 --port 8000 --reload
      "
    depends_on:
      redis:
        condition: service_healthy
      mlflow:
        condition: service_started

  # Celery Worker服务, 训练任务实际运行在此容器中
  worker:
    build:
      context: .
      dockerfile: ./worker/Dockerfile
    container_name: nylab_worker_debug
    ports:
      - "5679:5679"
    volumes:
      # 代码热重载
      - ./worker:/app/worker  
      - ./backend_common:/app/backend_common

      - shared_data:/data # 与web服务共享上传数据

    environment:
      PYTHONUNBUFFERED: "1"  # 确保实时日志输出
      MLFLOW_TRACKING_URI: http://mlflow:5000
      MINIO_ENDPOINT: minio:9000
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      REDIS_HOST: redis
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/1
      CELERY_CONCURRENCY: ${CELERY_CONCURRENCY}
    command: >
      sh -c "
      # 等待依赖服务就绪
      while ! nc -z $${REDIS_HOST:-redis} 6379; do sleep 2; done
      while ! nc -z minio 9000; do sleep 2; done
      while ! nc -z mlflow 5000; do sleep 2; done
      
      watchmedo auto-restart --directory=/app/worker --pattern='*.py' --recursive -- \
        celery -A worker.src.celery_app worker --loglevel=info --concurrency=$${CELERY_CONCURRENCY}
      
      # 保持容器运行
      tail -f /dev/null
      "
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: -1  # 表示使用所有可用 GPU
              capabilities: [gpu]

    depends_on:
      redis:
        condition: service_healthy
      web:
        condition: service_started

volumes:
  minio_data:
  shared_data: